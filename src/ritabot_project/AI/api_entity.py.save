# -*- coding: utf-8 -*

import time
# things we need for NLP
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.stem import SnowballStemmer
import json
# import for filtred words
from nltk.corpus import stopwords
from dateparser.search import search_dates
from flask import Flask
from flask import Flask, jsonify, request
application = Flask(__name__)

import re
from nltk.stem import SnowballStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
import pycountry
import phonenumbers
import pandas
import spacy
import conf


#load model spacy fot Named Entity Recognition
nlp = spacy.load("en_core_web_sm")


data_region_countries = pandas.read_csv(conf.PATH_PROJECT + "/region_countries/region_countries_csv.csv")
region_countries =list(data_region_countries["Code"])


#load alll zipcode of Tunisia country
data_zip_codes = pandas.read_csv(conf.PATH_PROJECT + "/ZipCode/zipcodes.csv",sep=';')
zip_codes = data_zip_codes["Code"]
zip_codes = set(zip_codes)


ingnorsWords = {'.', ',', ':', ';', '?', '!'}

#stemming Lemmatization sentence
def stemmer(sentence,langage):
    stemmer = SnowballStemmer(langage)
    words = word_tokenize(sentence)
    wordsFiltred = []
    for w in words:
        wordsFiltred.append(stemmer.stem(w.lower()))
    return wordsFiltred

#build a stemming sentence
def merge_words(words):
    stemmer_sentence = ""
    for w in words:
        if not (w in ingnorsWords):
            stemmer_sentence = stemmer_sentence + " " + w
        else:
            stemmer_sentence = stemmer_sentence + w
    return  stemmer_sentence.strip()

#extraction entity number from sentence
def extraction_number(sentence):

    entitys = []
    # numbers = re.findall('\d+', sentence)
    # numbers = re.findall('\d+', sentence)
    numbers = re.findall('^\d+|[^\\|^\w]\d+[^\w]|\d+$', sentence)
    for number in numbers:
        index_1 = sentence.find(number.strip())
        entitys.append({"start": index_1, "end": index_1 + len(number.strip()), "text": number.strip(), "type": "NUMBER","value": number.strip()})
    return entitys


#extraction entity date from sentence
def extraction_date(sentence):
    date_time= []
    dates = search_dates(sentence)
    if dates:
        for date in dates:
            print("date:", str(date[1].date()))
            if (len(date[0]) > 2):
                # date_time.append({"value": date[0], "date": str(date[1].date()), "time": str(date[1].time())})
                index = sentence.find(str((date[0])))
                date_time.append({"start": index , "end": index+len(str((date[0]))), "text": date[0],"type":"DATE" ,"value":str(date[1].date())})
    return date_time


#extraction entity date from sentence
def extraction_time(sentence):
    date_time = []
    dates = search_dates(sentence)
    if dates:
        for date in dates:
            print("time:", str(date[1].time()))
            if (len(date[0]) > 2):
                # date_time.append({"value": date[0], "date": str(date[1].date()), "time": str(date[1].time())})
                index = sentence.find(str((date[0])))
                date_time.append({"start": index , "end": index+len(str((date[0]))), "text": date[0],"type":"TIME" ,"value":str(date[1].time())})
    return date_time

#extraction entity country from sentence
def  entity_country(sentence):
    entitys =[]
    for country in pycountry.countries:
        index = sentence.find(country.name)
        if index != -1 :
            entitys.append({"start": index, "end": index + len(country.name), "text": country.name, "type": "COUNTRY", "value": country.name})

    return entitys

#extraction entity email adresses from sentence
def entity_email_addresses(sentence):
    entitys = []
    r = re.compile(r'[\w\.-]+@[\w\.-]+')
    emails = r.findall(sentence)
    for email in emails:
        index = sentence.find(email)
        if index != -1 :
            entitys.append({"start": index, "end": index + len(email), "text": email, "type": "EMAIL", "value": email})

    return entitys


#extraction entity email adresses from sentence
def entity_phone_number(sentence):
    entitys = []
    phones = []
    for region in region_countries:
        for match in phonenumbers.PhoneNumberMatcher(sentence, str(region)):
            index = str(match).find(")")
            phones.append(str(match)[index + 2:])
    phones = set(phones)
    for phone in phones:
        index = sentence.find(phone)
        if index != -1 :
            entitys.append({"start": index, "end": index + len(phone), "text": phone, "type": "PHONE", "value": phone})

    return entitys

#extraction entity URL from sentence
def entity_URL(sentence):
    entitys = []
    URL_REGEX = r"\b((?:https?://)?(?:(?:www\.)?(?:[\da-z\.-]+)\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\w\.-]*)*/?)\b"
    URLS = re.findall(URL_REGEX, sentence)
    for URL in URLS:
        index = sentence.find(URL)
        if index != -1:
            entitys.append({"start": index, "end": index + len(URL), "text": URL, "type": "URL", "value": URL})

    return entitys


#extraction entity ZipCodes from sentence
def entity_zip_code(sentence):
    entitys = []
    for zip_code in zip_codes:
        index = sentence.find(str(zip_code))
        if index != -1:
            entitys.append

#extraction entity PERSON from sentence
def entity_person(sentence):
    entitys = []
    doc = nlp(sentence)
    for ent in doc.ents:
        if ent.label_ == "PERSON":
            print(ent.text, ent.start_char, ent.end_char, ent.label_)
            entitys.append({"start": ent.start_char, "end": ent.end_char, "text": ent.text, "type": "PERSON", "value": ent.text})

    return  entitys


#extraction entity age from sentence
__vocalization = re.compile(r"[\u064b-\u064c-\u064d-\u064e-\u064f-\u0650-\u0651-\u0652]")  # ً، ٌ، ٍ، َ، ُ، ِ، ّ، ْ
__initial_hamzat = re.compile(r"^[\u0622\u0623\u0625]")  # أ، إ، آ
__arabic_punctuation_marks = re.compile(r"[\u060C-\u061B-\u061F]")  # ؛ ، ؟
__kasheeda = re.compile(r"[\u0640]")  # ـ tatweel/kasheeda
#normalized word type string
def __normalize_pre(token):

    # strip diacritics
    word = __vocalization.sub("", token)
    # strip kasheeda
    word = __kasheeda.sub("", word)
    # strip punctuation marks
    word = __arabic_punctuation_marks.sub("", word)
    word = __initial_hamzat.sub("\u0627", word)
    return word
age_units_en_fr = "(mois|months|month|ans|years|year)"
age_units_ar_tn = "(شهرا|اشهر|سنة|عام|عاما|سنوات)"
def entity_age(sentence,language):
    entitys = []
    words = []
    words_sentence = word_tokenize(sentence)
    for word in words_sentence:
        words.append(__normalize_pre(word))
    words = merge_words(words)
    if language =="arabic":
        entity = re.search(r'\d* ' + age_units_ar_tn, words, re.IGNORECASE)
    else:
        words = words.lower()
        entity = re.search(r'\d* ' + age_units_en_fr, words, re.IGNORECASE)
    if entity:
        entitys.append({"start": entity.start(), "end": entity.end(), "text":words[entity.start():entity.end()] , "type": "AGE",
                        "value": words[entity.start():entity.end()]})
    return  entitys


#extraction entity duration from sentence
duration_units_en_fr = "(mois|months|month|ans|years|year|jours|jour|days|day|minutes|munite|seconde|seconde|second|seconds|semaine|semaines|week|weeks)"
def entity_duration(sentence):
    entitys = []
    sentence = sentence.lower()
    entity = re.search(r'\d* ' + duration_units_en_fr, sentence, re.IGNORECASE)
    if entity:
        entitys.append(
            {"start": entity.start(), "end": entity.end(), "text": sentence[entity.start():entity.end()], "type": "DURATION",
             "value": sentence[entity.start():entity.end()]})
    return entitys


#extraction entity percent from sentence
percent_units_en_fr = "(pourcent| pourcent|%| %|percent| percent)"
def entity_percent(sentence):
    entitys = []
    sentence = sentence.lower()
    entity = re.search(r'((\d*)|(\d*[\,|\.]\d*))' + percent_units_en_fr, sentence, re.IGNORECASE)
    if entity:
        entitys.append(
            {"start": entity.start(), "end": entity.end(), "text": sentence[entity.start():entity.end()], "type": "PERCENT",
             "value": sentence[entity.start():entity.end()]})
    return entitys


#extraction entity wheight from sentence
wheight_units_fr = "(milligramme|milligrammes|gramme|grammes|kilogramme|kilogrammes|tonne|tonnes|mg|g|kg|t)"
wheight_units_en ="(milligram|milligrams|gram|grams|kilogram|kilograms|tonne|tonnes|mg|g|kg|t)"
def entity_wheight(sentence, language):
    entitys = []
    sentence_origin = sentence
    sentence = sentence.lower()
    if language == "french":
        sentence = sentence.replace('é', 'e')
        sentence = sentence.replace('è', 'e')
        sentence = sentence.replace('ê', 'e')
        entity = re.search(r'((\d*)|(\d*[\,|\.]\d*)) ' + wheight_units_fr, sentence, re.IGNORECASE)
    else:
        entity = re.search(r'((\d*)|(\d*[\,|\.]\d*)) ' + wheight_units_en, sentence, re.IGNORECASE)
    if entity:
        entitys.append(
            {"start": entity.start(), "end": entity.end(), "text": sentence_origin[entity.start():entity.end()], "type": "WHEIGHT",
             "value": sentence_origin[entity.start():entity.end()]})
    return entitys

#extraction entity lengh from sentence
lengh_units_fr = "(millimetre|millimetres|centimetre|centimetres|decimetre|decimetres|metre|metres|decametre|decametres|hectometre|hectometres|kilometre|kilometres|mm|cm|dm|m|dam|hm|km)"
lengh_units_en ="(millimeter|millimeters|centimeter|centimeters|decimeter|decimeters|meter|meters|decameter|decameters|hectometer|hectometers|kilometer|kilometers|mm|cm|dm|m|dam|hm|km)"
def entity_lengh(sentence, language):
    entitys = []
    sentence_origin = sentence
    sentence = sentence.lower()
    if language == "french":
        sentence = sentence.replace('é', 'e')
        sentence = sentence.replace('è', 'e')
        sentence = sentence.replace('ê', 'e')
        entity = re.search(r'((\d*)|(\d*[\,|\.]\d*)) ' + lengh_units_fr, sentence, re.IGNORECASE)
    else:
        entity = re.search(r'((\d*)|(\d*[\,|\.]\d*)) ' + lengh_units_en, sentence, re.IGNORECASE)
    if entity:
        entitys.append(
            {"start": entity.start(), "end": entity.end(), "text": sentence_origin[entity.start():entity.end()], "type": "LENGH",
             "value": sentence_origin[entity.start():entity.end()]})
    return entitys




#extraction customers entity from sentence
def entity_search(sentence,ents,language):
    entitys =[]
    words_sentence = word_tokenize(sentence)
    words_sentence_stemmer = stemmer(sentence,language)
    wordsFiltred_sentence = merge_words(stemmer(sentence,language))
    tags = ents.keys()
    for tag in tags:
        for indice in ents[tag]["value"]:
            words_entity_stemmer = stemmer(indice,language)
            wordsFiltred_entity = merge_words(stemmer(indice,language))
            index = wordsFiltred_sentence.find(wordsFiltred_entity)
            if (index != -1):
                start = index
                l = ""
                for element in words_entity_stemmer:
                    l = l + " " + words_sentence[words_sentence_stemmer.index(element)]
                index_1 = sentence.find(l.strip())
                entitys.append({"start":index_1,"end":index_1+len(l.strip()),"text":l.strip(),"type":tag,"value":l.strip()})


    return  entitys




@application.route('/entity', methods=['GET','POST'])
def get_entity():
    data = request.json
    print("test",data)
    language = data["LANGUAGE"]
    sentence = data["SENTENCE"]
    del data["LANGUAGE"]
    del data["SENTENCE"]
    # del data["responses"]

    start = time.time()
    entitys = []
    try:
        if(data["NUMBER"]["value"]):
            entitys = entitys + extraction_number(sentence)
        if(data["TIME"]["value"]):
            entitys = entitys + extraction_time(sentence)
        if (data["DATE"]["value"]):
            entitys = entitys + extraction_date(sentence)
        if (data["COUNTRY"]["value"]):
            entitys = entitys + entity_country(sentence)
        if (data["EMAIL"]["value"]):
            entitys = entitys + entity_email_addresses(sentence)
        if (data["PHONE"]["value"]):
            entitys = entitys + entity_phone_number(sentence)
        if (data["URL"]["value"]):
            entitys = entitys + entity_URL(sentence)
        if (data["ZIPCODE"]["value"]):
            entitys = entitys + entity_zip_code(sentence)
        if (data["PERSON"]["value"]):
            entitys = entitys + entity_person(sentence)
        if (data["AGE"]["value"]):
            entitys = entitys + entity_age(sentence, language)
        if (data["DURATION"]["value"]):
            entitys = entitys + entity_duration(sentence)
        if (data["PERCENT"]["value"]):
            entitys = entitys + entity_percent(sentence)
        if (data["WHEIGHT"]["value"]):
            entitys = entitys + entity_wheight(sentence, language)
        if (data["LENGH"]["value"]):
            entitys = entitys + entity_lengh(sentence, language)

        del data["NUMBER"]
        del data["TIME"]
        del data["DATE"]
        del data["COUNTRY"]
        del data["EMAIL"]
        del data["PHONE"]
        del data["URL"]
        del data["ZIPCODE"]
        del data["PERSON"]
        del data["AGE"]
        del data["DURATION"]
        del data["PERCENT"]
        del data["WHEIGHT"]
        del data["LENGH"]


        entitys = entitys  + entity_search(sentence,data,language)
    except:
        pass
    end = time.time()
    print("temps d'execution est {0}".format((end-start)))
    print("enttt:",entitys)
    return jsonify({"data":entitys})

#if __name__ == '__main__':
#    app.run(host='0.0.0.0', threaded=True, port=5000, debug=True)



